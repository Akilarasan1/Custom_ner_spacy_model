{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3d7ee34-f57b-4fea-b3e6-f80d68907115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER label counts:\n",
      "vehicle_type: 1521\n",
      "vehicle_location: 11394\n",
      "vehicle_color: 19312\n",
      "vehicle_range: 188\n",
      "vehicle_orientation: 8286\n",
      "vehicle_velocity: 8141\n",
      "vehicle_brand: 17872\n",
      "vehicle_model: 17872\n",
      "vehicle_type-suv: 2564\n",
      "vehicle_type-sedan: 3909\n",
      "vehicle_type-hatchback: 1615\n",
      "vehicle_type-sports_car: 1646\n",
      "vehicle_type-coupe: 945\n",
      "vehicle_type-bus: 462\n",
      "vehicle_type-vintage_car: 1412\n",
      "vehicle_type-motorcycle: 2592\n",
      "vehicle_type-truck: 859\n",
      "vehicle_type-roadster: 373\n",
      "vehicle_type-van: 556\n",
      "vehicle_type-mpv: 608\n",
      "vehicle_type-estate_car: 521\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "from collections import Counter\n",
    "\n",
    "train_file_path = r\"C:\\Users\\akilarasan.p\\Downloads\\DATASET\\NLP-data\\FindVehicle_train.jsonl\"\n",
    "\n",
    "with jsonlines.open(train_file_path) as reader:\n",
    "    records = list(reader)\n",
    "\n",
    "label_counter = Counter()\n",
    "\n",
    "for record in records:\n",
    "    for label in record.get('ner_label', []):\n",
    "        label_type = label[0]  # The first element is the label type\n",
    "        label_counter[label_type] += 1\n",
    "\n",
    "print(\"NER label counts:\")\n",
    "for label_type, count in label_counter.items():\n",
    "    print(f\"{label_type}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f9ccae9-fea7-46ce-8168-a18c72ab9c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862da543d47e4ca58d6c71fdbb6a054f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/21565 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done. Total samples: 21565\n",
      "Labels: ['O', 'B-vehicle_brand', 'I-vehicle_brand', 'B-vehicle_color', 'I-vehicle_color', 'B-vehicle_location', 'I-vehicle_location', 'B-vehicle_model', 'I-vehicle_model', 'B-vehicle_orientation', 'I-vehicle_orientation', 'B-vehicle_range', 'I-vehicle_range', 'B-vehicle_type', 'I-vehicle_type', 'B-vehicle_type-bus', 'I-vehicle_type-bus', 'B-vehicle_type-coupe', 'I-vehicle_type-coupe', 'B-vehicle_type-estate_car', 'I-vehicle_type-estate_car', 'B-vehicle_type-hatchback', 'I-vehicle_type-hatchback', 'B-vehicle_type-motorcycle', 'I-vehicle_type-motorcycle', 'B-vehicle_type-mpv', 'I-vehicle_type-mpv', 'B-vehicle_type-roadster', 'I-vehicle_type-roadster', 'B-vehicle_type-sedan', 'I-vehicle_type-sedan', 'B-vehicle_type-sports_car', 'I-vehicle_type-sports_car', 'B-vehicle_type-suv', 'I-vehicle_type-suv', 'B-vehicle_type-truck', 'I-vehicle_type-truck', 'B-vehicle_type-van', 'I-vehicle_type-van', 'B-vehicle_type-vintage_car', 'I-vehicle_type-vintage_car', 'B-vehicle_velocity', 'I-vehicle_velocity']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import BertTokenizerFast\n",
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "# === Setup ===\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
    "input_file = r\"C:\\Users\\akilarasan.p\\Downloads\\DATASET\\NLP-data\\FindVehicle_train.jsonl\"\n",
    "output_file = r\"C:\\Users\\akilarasan.p\\Downloads\\DATASET\\NLP-data\\bio_tagged_dataset.json\"\n",
    "\n",
    "# === Collect unique labels for tag mapping ===\n",
    "all_labels = set()\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        sample = json.loads(line)\n",
    "        for label in sample[\"ner_label\"]:\n",
    "            all_labels.add(label[0])\n",
    "\n",
    "# Add BIO prefix and map to IDs\n",
    "labels_list = sorted(list(all_labels))\n",
    "bio_labels = [\"O\"] + [f\"{prefix}-{label}\" for label in labels_list for prefix in [\"B\", \"I\"]]\n",
    "label2id = {label: i for i, label in enumerate(bio_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# === Conversion ===\n",
    "def convert_sample(sample):\n",
    "    text = sample[\"data\"]\n",
    "    entities = sample[\"ner_label\"]\n",
    "\n",
    "    # Tokenize with offset mapping\n",
    "    encoding = tokenizer(text, return_offsets_mapping=True, truncation=True)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"])\n",
    "    offsets = encoding[\"offset_mapping\"]\n",
    "\n",
    "    tags = [\"O\"] * len(tokens)\n",
    "\n",
    "    for entity in entities:\n",
    "        label, start_char, end_char = entity[0], entity[1], entity[2]\n",
    "\n",
    "        for i, (token_start, token_end) in enumerate(offsets):\n",
    "            if token_start >= end_char:\n",
    "                break\n",
    "            if token_end <= start_char:\n",
    "                continue\n",
    "\n",
    "            if token_start >= start_char and token_end <= end_char:\n",
    "                if tags[i] == \"O\":\n",
    "                    tags[i] = f\"B-{label}\" if token_start == start_char else f\"I-{label}\"\n",
    "\n",
    "    tag_ids = [label2id[tag] for tag in tags]\n",
    "    return {\n",
    "        \"tokens\": tokens,\n",
    "        \"ner_tags\": tag_ids\n",
    "    }\n",
    "\n",
    "# === Process All Samples ===\n",
    "converted = []\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        sample = json.loads(line)\n",
    "        converted.append(convert_sample(sample))\n",
    "\n",
    "# === Save to File (optional) ===\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(converted, f, indent=2)\n",
    "\n",
    "# === Optional: Convert to Hugging Face Dataset ===\n",
    "dataset = Dataset.from_list(converted)\n",
    "dataset.save_to_disk(\"bio_tagged_hf_dataset\")\n",
    "\n",
    "print(f\"✅ Done. Total samples: {len(converted)}\")\n",
    "print(f\"Labels: {bio_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "501aa836-d5ed-434c-8a4e-9bba5f172553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers datasets seqeval scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4a06b95-6485-4160-a441-86964af80132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\akilarasan.p\\.conda\\envs\\openai\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seqeval'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset, ClassLabel, Dataset\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclass_weight\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_class_weight\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mseqeval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, f1_score, precision_score, recall_score\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seqeval'"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, ClassLabel, Dataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# === Load Data ===\n",
    "with open(\"bio_tagged_dataset.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "dataset = Dataset.from_list(data)\n",
    "label_list = sorted(set(tag for row in data for tag in row[\"ner_tags\"]))\n",
    "id2label = {id: f\"LABEL_{id}\" for id in label_list}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# === Tokenizer & Model ===\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_checkpoint)\n",
    "model = BertForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list), id2label=id2label, label2id=label2id)\n",
    "\n",
    "# === Compute Class Weights ===\n",
    "all_labels = [tag for row in data for tag in row[\"ner_tags\"]]\n",
    "weights = compute_class_weight('balanced', classes=np.array(label_list), y=all_labels)\n",
    "class_weights = torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "# Inject class weights into loss\n",
    "def compute_loss_with_weights(model, inputs, return_outputs=False):\n",
    "    labels = inputs.get(\"labels\")\n",
    "    outputs = model(**inputs)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights.to(outputs.logits.device))\n",
    "    loss = loss_fct(outputs.logits.view(-1, model.num_labels), labels.view(-1))\n",
    "    return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "model.compute_loss = compute_loss_with_weights\n",
    "\n",
    "# === Tokenize Inputs (they’re already tokenized) ===\n",
    "def tokenize_and_align_labels(example):\n",
    "    return {\"input_ids\": tokenizer.convert_tokens_to_ids(example[\"tokens\"])}\n",
    "\n",
    "dataset = dataset.map(tokenize_and_align_labels)\n",
    "\n",
    "# === Train/Test Split ===\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "# === Metrics ===\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_labels = [[id2label.get(l, \"O\") for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [[id2label.get(p, \"O\") for (p, l) in zip(pred, label) if l != -100]\n",
    "                        for pred, label in zip(predictions, labels)]\n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "    }\n",
    "\n",
    "# === Training Arguments ===\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./ner_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# === Trainer ===\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# === Train ===\n",
    "trainer.train()\n",
    "\n",
    "# === Save Model ===\n",
    "trainer.save_model(\"ner_model\")\n",
    "tokenizer.save_pretrained(\"ner_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a128f68e-376f-4bab-af1a-3af0ca2c0974",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "178acc5df024476e84bcadc2e2b6c959": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_b093d731eca547229ea927f64f65d00d",
       "style": "IPY_MODEL_71a1b14a2bae4721a299d76fdacd5952",
       "value": "Saving the dataset (1/1 shards): 100%"
      }
     },
     "19b909f751f34238a904eeecdfa0f2d1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1e325c9b9756470dba31c9ff3f7ee393": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1f8cdda7d89a4844b3be83e02fb352b6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "235c96d19a2a46978bbce9e549aadb3e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "29e8707641d4425fbb8748bc4b1ddbc5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_235c96d19a2a46978bbce9e549aadb3e",
       "max": 21565,
       "style": "IPY_MODEL_3f8e518c94b14176b33c2e6184139e91",
       "value": 21565
      }
     },
     "389fcfc9ae564c5982146cc2f2669510": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_1e325c9b9756470dba31c9ff3f7ee393",
       "style": "IPY_MODEL_fddaf7e367a6438caee139eed0e78437",
       "value": " 21565/21565 [00:00&lt;00:00, 339689.59 examples/s]"
      }
     },
     "3e466730b5c04e79a9ba1e8579ff87ae": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3f8e518c94b14176b33c2e6184139e91": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "4d7db04cb4394a92bb2234af857f8093": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5db4036ba01c44b197659415b08ba51f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "71a1b14a2bae4721a299d76fdacd5952": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "85238e58f956438a9892cefa85ec0dcf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "862da543d47e4ca58d6c71fdbb6a054f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_178acc5df024476e84bcadc2e2b6c959",
        "IPY_MODEL_95de0040a47842d19abf928b1e771fb8",
        "IPY_MODEL_94e0ab2e5a8a42b294fae9c2088e058b"
       ],
       "layout": "IPY_MODEL_9ffdbed2282f45f0acdc1c57fa3e3ea3"
      }
     },
     "94e0ab2e5a8a42b294fae9c2088e058b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_3e466730b5c04e79a9ba1e8579ff87ae",
       "style": "IPY_MODEL_19b909f751f34238a904eeecdfa0f2d1",
       "value": " 21565/21565 [00:00&lt;00:00, 2161.13 examples/s]"
      }
     },
     "95de0040a47842d19abf928b1e771fb8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_1f8cdda7d89a4844b3be83e02fb352b6",
       "max": 21565,
       "style": "IPY_MODEL_85238e58f956438a9892cefa85ec0dcf",
       "value": 21565
      }
     },
     "9ffdbed2282f45f0acdc1c57fa3e3ea3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b093d731eca547229ea927f64f65d00d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c2b1b73809f04deeb64b3a55dc31d3a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ec93a63acdbe414c8496361101188f23",
        "IPY_MODEL_29e8707641d4425fbb8748bc4b1ddbc5",
        "IPY_MODEL_389fcfc9ae564c5982146cc2f2669510"
       ],
       "layout": "IPY_MODEL_4d7db04cb4394a92bb2234af857f8093"
      }
     },
     "ec93a63acdbe414c8496361101188f23": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f9e51077bc5f43a4a9a2777c2929ef5c",
       "style": "IPY_MODEL_5db4036ba01c44b197659415b08ba51f",
       "value": "Saving the dataset (1/1 shards): 100%"
      }
     },
     "f9e51077bc5f43a4a9a2777c2929ef5c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fddaf7e367a6438caee139eed0e78437": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
